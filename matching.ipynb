{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import pickle\n",
        "\n",
        "#import Levenshtein\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Upload json files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#files= ['0.json', '1.json', '2.json', '3.json', '4.json','42.json', '15693.json']\n",
        "files = ['sample.json']\n",
        "# create an empty list to store the DataFrames\n",
        "dfsList = [pd.read_json(file, orient='records') for file in files]\n",
        "\n",
        "\n",
        "# combine all DataFrames into a single DataFrame\n",
        "crossrefDF = pd.concat(dfsList, ignore_index=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data preparation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "noAuthors = [i for i in range(len(crossrefDF)) if 'author' not in crossrefDF['items'][i]]\n",
        "\n",
        "Authors = [i for i in range(len(crossrefDF)) if 'author'  in crossrefDF['items'][i]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rows with authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "crossrefAuth = crossrefDF.iloc[Authors].copy()\n",
        "\n",
        "crossrefAuth.reset_index(inplace= True)\n",
        "crossrefAuth.drop(columns = ['index'], inplace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>items</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1504/ijil.2023.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'abstract': '&lt;jats:p&gt;Product management is a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1021/acsami.2c18...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1149/1.1393434',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1021/acsomega.2c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>882</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1021/jm020002p',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.15278/isms.2022....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>884</th>\n",
              "      <td>{'abstract': '&lt;jats:p&gt;In a 21st century classr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>885</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1016/j.jmrt.2022...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1016/0012-365x(9...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>887 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 items\n",
              "0    {'URL': 'http://dx.doi.org/10.1504/ijil.2023.1...\n",
              "1    {'abstract': '<jats:p>Product management is a ...\n",
              "2    {'URL': 'http://dx.doi.org/10.1021/acsami.2c18...\n",
              "3    {'URL': 'http://dx.doi.org/10.1149/1.1393434',...\n",
              "4    {'URL': 'http://dx.doi.org/10.1021/acsomega.2c...\n",
              "..                                                 ...\n",
              "882  {'URL': 'http://dx.doi.org/10.1021/jm020002p',...\n",
              "883  {'URL': 'http://dx.doi.org/10.15278/isms.2022....\n",
              "884  {'abstract': '<jats:p>In a 21st century classr...\n",
              "885  {'URL': 'http://dx.doi.org/10.1016/j.jmrt.2022...\n",
              "886  {'URL': 'http://dx.doi.org/10.1016/0012-365x(9...\n",
              "\n",
              "[887 rows x 1 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "crossrefAuth"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract 'DOI', authors --- number of authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "crossrefAuth.loc[:, 'DOI'] = crossrefAuth['items'].apply(lambda x: x['DOI'])\n",
        "crossrefAuth.loc[:,'authors'] = crossrefAuth['items'].apply(lambda x: x['author'])\n",
        "\n",
        "numAuthors = [len(crossrefAuth.iloc[i]['authors']) for i in range(len(crossrefAuth))]\n",
        "\n",
        "crossrefAuth.loc[:,'# authors'] = numAuthors"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract 'affiliations' --- number of affiliations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getAff(k):\n",
        "   return [crossrefAuth['authors'][k][j]['affiliation'] for j in range(len(crossrefAuth['authors'][k]))]\n",
        "    \n",
        "Affiliations = [getAff(k) for k in range(len(crossrefAuth))]\n",
        "\n",
        "crossrefAuth.loc[:,'affiliations'] = Affiliations\n",
        "\n",
        "numAffil = [len(Affiliations[i]) for i in range(len(crossrefAuth))]\n",
        "\n",
        "crossrefAuth.loc[:,'# Affil'] = numAffil"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean 'empty' affiliations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "possibleEmptyAff = []\n",
        "\n",
        "for k in range(len(crossrefAuth)):\n",
        "    if len(crossrefAuth['affiliations'][k][0]) == 0:\n",
        "        possibleEmptyAff.append(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "610"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(possibleEmptyAff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "nonEmptyAff = []\n",
        "\n",
        "for k in possibleEmptyAff:\n",
        "    for j in range(len(crossrefAuth['affiliations'].iloc[k])):\n",
        "        if len(crossrefAuth['affiliations'].iloc[k][j]) != 0:\n",
        "            nonEmptyAff.append(k)\n",
        "    \n",
        "FinalEmptyyAff =  [x for x in possibleEmptyAff if x not in nonEmptyAff] \n",
        "FinalNonEmptyAff = [x for x in range(len(crossrefAuth)) if x not in FinalEmptyyAff]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# doiDF: crossrefAuth subdataframe with nonpempty affiliation lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "doiDF = crossrefAuth.iloc[FinalNonEmptyAff].copy()\n",
        "doiDF.reset_index(inplace = True)\n",
        "doiDF.drop(columns = ['index'], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>items</th>\n",
              "      <th>DOI</th>\n",
              "      <th>authors</th>\n",
              "      <th># authors</th>\n",
              "      <th>affiliations</th>\n",
              "      <th># Affil</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1021/acsami.2c18...</td>\n",
              "      <td>10.1021/acsami.2c18397</td>\n",
              "      <td>[{'given': 'Guoxian', 'family': 'Zhang', 'sequ...</td>\n",
              "      <td>8</td>\n",
              "      <td>[[{'name': 'Key Laboratory of Special Function...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1021/acsomega.2c...</td>\n",
              "      <td>10.1021/acsomega.2c04273</td>\n",
              "      <td>[{'given': 'Junlong', 'family': 'Han', 'sequen...</td>\n",
              "      <td>7</td>\n",
              "      <td>[[{'name': 'School of Mechanical Engineering a...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'abstract': '&lt;jats:p&gt;The article provides a t...</td>\n",
              "      <td>10.54891/2786-7005-2022-1-13</td>\n",
              "      <td>[{'ORCID': 'http://orcid.org/0000-0002-1686-19...</td>\n",
              "      <td>2</td>\n",
              "      <td>[[{'name': 'Communal Institution of Higher Edu...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'abstract': '&lt;jats:title&gt;Abstract&lt;/jats:title...</td>\n",
              "      <td>10.4049/jimmunol.158.8.3587</td>\n",
              "      <td>[{'given': 'S J', 'family': 'Gobin', 'sequence...</td>\n",
              "      <td>4</td>\n",
              "      <td>[[{'name': 'Department of Immunohaematology an...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1061/(asce)0887-...</td>\n",
              "      <td>10.1061/(asce)0887-3828(2002)16:3(98)</td>\n",
              "      <td>[{'given': 'Norbert J.', 'family': 'Delatte', ...</td>\n",
              "      <td>2</td>\n",
              "      <td>[[{'name': 'Assistant Professor, Dept. of Civi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>{'abstract': '&lt;jats:p&gt;The authors highlight th...</td>\n",
              "      <td>10.4018/978-1-6684-7593-5.ch044</td>\n",
              "      <td>[{'given': 'Renuka', 'family': 'Garg', 'sequen...</td>\n",
              "      <td>2</td>\n",
              "      <td>[[{'name': 'Veer Narmad South Gujarat Universi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>{'abstract': '&lt;jats:title&gt;Abstract&lt;/jats:title...</td>\n",
              "      <td>10.4049/jimmunol.159.11.5372</td>\n",
              "      <td>[{'given': 'C J', 'family': 'Howard', 'sequenc...</td>\n",
              "      <td>6</td>\n",
              "      <td>[[{'name': 'The Institute for Animal Health, C...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>{'abstract': '&lt;jats:p&gt;: Overcrowding and house...</td>\n",
              "      <td>10.18231/j.jchm.2022.037</td>\n",
              "      <td>[{'given': 'Ravikant Rambhai', 'family': 'Pate...</td>\n",
              "      <td>4</td>\n",
              "      <td>[[], [{'name': 'GMERS Medical College, Valsad,...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>{'abstract': '&lt;jats:title&gt;Abstract&lt;/jats:title...</td>\n",
              "      <td>10.4049/jimmunol.162.12.6967</td>\n",
              "      <td>[{'given': 'Chris A.', 'family': 'Benedict', '...</td>\n",
              "      <td>8</td>\n",
              "      <td>[[{'name': '*Division of Molecular Immunology,...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1021/jm020002p',...</td>\n",
              "      <td>10.1021/jm020002p</td>\n",
              "      <td>[{'given': 'Rachel A.', 'family': 'Powers', 's...</td>\n",
              "      <td>2</td>\n",
              "      <td>[[{'name': 'Department of Molecular Pharmacolo...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>278 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 items  \\\n",
              "0    {'URL': 'http://dx.doi.org/10.1021/acsami.2c18...   \n",
              "1    {'URL': 'http://dx.doi.org/10.1021/acsomega.2c...   \n",
              "2    {'abstract': '<jats:p>The article provides a t...   \n",
              "3    {'abstract': '<jats:title>Abstract</jats:title...   \n",
              "4    {'URL': 'http://dx.doi.org/10.1061/(asce)0887-...   \n",
              "..                                                 ...   \n",
              "273  {'abstract': '<jats:p>The authors highlight th...   \n",
              "274  {'abstract': '<jats:title>Abstract</jats:title...   \n",
              "275  {'abstract': '<jats:p>: Overcrowding and house...   \n",
              "276  {'abstract': '<jats:title>Abstract</jats:title...   \n",
              "277  {'URL': 'http://dx.doi.org/10.1021/jm020002p',...   \n",
              "\n",
              "                                       DOI  \\\n",
              "0                   10.1021/acsami.2c18397   \n",
              "1                 10.1021/acsomega.2c04273   \n",
              "2             10.54891/2786-7005-2022-1-13   \n",
              "3              10.4049/jimmunol.158.8.3587   \n",
              "4    10.1061/(asce)0887-3828(2002)16:3(98)   \n",
              "..                                     ...   \n",
              "273        10.4018/978-1-6684-7593-5.ch044   \n",
              "274           10.4049/jimmunol.159.11.5372   \n",
              "275               10.18231/j.jchm.2022.037   \n",
              "276           10.4049/jimmunol.162.12.6967   \n",
              "277                      10.1021/jm020002p   \n",
              "\n",
              "                                               authors  # authors  \\\n",
              "0    [{'given': 'Guoxian', 'family': 'Zhang', 'sequ...          8   \n",
              "1    [{'given': 'Junlong', 'family': 'Han', 'sequen...          7   \n",
              "2    [{'ORCID': 'http://orcid.org/0000-0002-1686-19...          2   \n",
              "3    [{'given': 'S J', 'family': 'Gobin', 'sequence...          4   \n",
              "4    [{'given': 'Norbert J.', 'family': 'Delatte', ...          2   \n",
              "..                                                 ...        ...   \n",
              "273  [{'given': 'Renuka', 'family': 'Garg', 'sequen...          2   \n",
              "274  [{'given': 'C J', 'family': 'Howard', 'sequenc...          6   \n",
              "275  [{'given': 'Ravikant Rambhai', 'family': 'Pate...          4   \n",
              "276  [{'given': 'Chris A.', 'family': 'Benedict', '...          8   \n",
              "277  [{'given': 'Rachel A.', 'family': 'Powers', 's...          2   \n",
              "\n",
              "                                          affiliations  # Affil  \n",
              "0    [[{'name': 'Key Laboratory of Special Function...        8  \n",
              "1    [[{'name': 'School of Mechanical Engineering a...        7  \n",
              "2    [[{'name': 'Communal Institution of Higher Edu...        2  \n",
              "3    [[{'name': 'Department of Immunohaematology an...        4  \n",
              "4    [[{'name': 'Assistant Professor, Dept. of Civi...        2  \n",
              "..                                                 ...      ...  \n",
              "273  [[{'name': 'Veer Narmad South Gujarat Universi...        2  \n",
              "274  [[{'name': 'The Institute for Animal Health, C...        6  \n",
              "275  [[], [{'name': 'GMERS Medical College, Valsad,...        4  \n",
              "276  [[{'name': '*Division of Molecular Immunology,...        8  \n",
              "277  [[{'name': 'Department of Molecular Pharmacolo...        2  \n",
              "\n",
              "[278 rows x 6 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doiDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>items</th>\n",
              "      <th>DOI</th>\n",
              "      <th>authors</th>\n",
              "      <th># authors</th>\n",
              "      <th>affiliations</th>\n",
              "      <th># Affil</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1021/acsami.2c18...</td>\n",
              "      <td>10.1021/acsami.2c18397</td>\n",
              "      <td>[{'given': 'Guoxian', 'family': 'Zhang', 'sequ...</td>\n",
              "      <td>8</td>\n",
              "      <td>[[{'name': 'Key Laboratory of Special Function...</td>\n",
              "      <td>8</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1021/acsomega.2c...</td>\n",
              "      <td>10.1021/acsomega.2c04273</td>\n",
              "      <td>[{'given': 'Junlong', 'family': 'Han', 'sequen...</td>\n",
              "      <td>7</td>\n",
              "      <td>[[{'name': 'School of Mechanical Engineering a...</td>\n",
              "      <td>7</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'abstract': '&lt;jats:p&gt;The article provides a t...</td>\n",
              "      <td>10.54891/2786-7005-2022-1-13</td>\n",
              "      <td>[{'ORCID': 'http://orcid.org/0000-0002-1686-19...</td>\n",
              "      <td>2</td>\n",
              "      <td>[[{'name': 'Communal Institution of Higher Edu...</td>\n",
              "      <td>2</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'abstract': '&lt;jats:title&gt;Abstract&lt;/jats:title...</td>\n",
              "      <td>10.4049/jimmunol.158.8.3587</td>\n",
              "      <td>[{'given': 'S J', 'family': 'Gobin', 'sequence...</td>\n",
              "      <td>4</td>\n",
              "      <td>[[{'name': 'Department of Immunohaematology an...</td>\n",
              "      <td>4</td>\n",
              "      <td>1997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1061/(asce)0887-...</td>\n",
              "      <td>10.1061/(asce)0887-3828(2002)16:3(98)</td>\n",
              "      <td>[{'given': 'Norbert J.', 'family': 'Delatte', ...</td>\n",
              "      <td>2</td>\n",
              "      <td>[[{'name': 'Assistant Professor, Dept. of Civi...</td>\n",
              "      <td>2</td>\n",
              "      <td>2002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.15280/jlm.2022.1...</td>\n",
              "      <td>10.15280/jlm.2022.12.3.138</td>\n",
              "      <td>[{'given': 'Abhishek', 'family': 'Sharma', 'se...</td>\n",
              "      <td>3</td>\n",
              "      <td>[[{'name': 'Department of Paediatric and Neona...</td>\n",
              "      <td>3</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.25253/99.2022244...</td>\n",
              "      <td>10.25253/99.2022244.11</td>\n",
              "      <td>[{'ORCID': 'http://orcid.org/0000-0002-0585-54...</td>\n",
              "      <td>1</td>\n",
              "      <td>[[{'name': 'Aksaray University, Türkiye'}]]</td>\n",
              "      <td>1</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>{'abstract': '&lt;jats:title&gt;Abstract&lt;/jats:title...</td>\n",
              "      <td>10.4049/jimmunol.159.11.5535</td>\n",
              "      <td>[{'given': 'F H', 'family': 'Amante', 'sequenc...</td>\n",
              "      <td>4</td>\n",
              "      <td>[[{'name': 'The Cooperative Research Center fo...</td>\n",
              "      <td>4</td>\n",
              "      <td>1997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>{'URL': 'http://dx.doi.org/10.1080/23322039.20...</td>\n",
              "      <td>10.1080/23322039.2022.2161774</td>\n",
              "      <td>[{'ORCID': 'http://orcid.org/0000-0002-9452-09...</td>\n",
              "      <td>2</td>\n",
              "      <td>[[{'name': 'Pan African University Institute o...</td>\n",
              "      <td>2</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>{'abstract': '&lt;jats:p&gt;Cryptocurrencies show so...</td>\n",
              "      <td>10.22495/rgcv12i4p5</td>\n",
              "      <td>[{'ORCID': 'http://orcid.org/0000-0001-9173-56...</td>\n",
              "      <td>3</td>\n",
              "      <td>[[{'name': 'Masaryk University'}], [{'name': '...</td>\n",
              "      <td>3</td>\n",
              "      <td>2022</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               items  \\\n",
              "0  {'URL': 'http://dx.doi.org/10.1021/acsami.2c18...   \n",
              "1  {'URL': 'http://dx.doi.org/10.1021/acsomega.2c...   \n",
              "2  {'abstract': '<jats:p>The article provides a t...   \n",
              "3  {'abstract': '<jats:title>Abstract</jats:title...   \n",
              "4  {'URL': 'http://dx.doi.org/10.1061/(asce)0887-...   \n",
              "5  {'URL': 'http://dx.doi.org/10.15280/jlm.2022.1...   \n",
              "6  {'URL': 'http://dx.doi.org/10.25253/99.2022244...   \n",
              "7  {'abstract': '<jats:title>Abstract</jats:title...   \n",
              "8  {'URL': 'http://dx.doi.org/10.1080/23322039.20...   \n",
              "9  {'abstract': '<jats:p>Cryptocurrencies show so...   \n",
              "\n",
              "                                     DOI  \\\n",
              "0                 10.1021/acsami.2c18397   \n",
              "1               10.1021/acsomega.2c04273   \n",
              "2           10.54891/2786-7005-2022-1-13   \n",
              "3            10.4049/jimmunol.158.8.3587   \n",
              "4  10.1061/(asce)0887-3828(2002)16:3(98)   \n",
              "5             10.15280/jlm.2022.12.3.138   \n",
              "6                 10.25253/99.2022244.11   \n",
              "7           10.4049/jimmunol.159.11.5535   \n",
              "8          10.1080/23322039.2022.2161774   \n",
              "9                    10.22495/rgcv12i4p5   \n",
              "\n",
              "                                             authors  # authors  \\\n",
              "0  [{'given': 'Guoxian', 'family': 'Zhang', 'sequ...          8   \n",
              "1  [{'given': 'Junlong', 'family': 'Han', 'sequen...          7   \n",
              "2  [{'ORCID': 'http://orcid.org/0000-0002-1686-19...          2   \n",
              "3  [{'given': 'S J', 'family': 'Gobin', 'sequence...          4   \n",
              "4  [{'given': 'Norbert J.', 'family': 'Delatte', ...          2   \n",
              "5  [{'given': 'Abhishek', 'family': 'Sharma', 'se...          3   \n",
              "6  [{'ORCID': 'http://orcid.org/0000-0002-0585-54...          1   \n",
              "7  [{'given': 'F H', 'family': 'Amante', 'sequenc...          4   \n",
              "8  [{'ORCID': 'http://orcid.org/0000-0002-9452-09...          2   \n",
              "9  [{'ORCID': 'http://orcid.org/0000-0001-9173-56...          3   \n",
              "\n",
              "                                        affiliations  # Affil  year  \n",
              "0  [[{'name': 'Key Laboratory of Special Function...        8  2022  \n",
              "1  [[{'name': 'School of Mechanical Engineering a...        7  2022  \n",
              "2  [[{'name': 'Communal Institution of Higher Edu...        2  2022  \n",
              "3  [[{'name': 'Department of Immunohaematology an...        4  1997  \n",
              "4  [[{'name': 'Assistant Professor, Dept. of Civi...        2  2002  \n",
              "5  [[{'name': 'Department of Paediatric and Neona...        3  2022  \n",
              "6        [[{'name': 'Aksaray University, Türkiye'}]]        1  2022  \n",
              "7  [[{'name': 'The Cooperative Research Center fo...        4  1997  \n",
              "8  [[{'name': 'Pan African University Institute o...        2  2022  \n",
              "9  [[{'name': 'Masaryk University'}], [{'name': '...        3  2022  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "year = [(doiDF['items'].iloc[i]['issued']['date-parts'][0][0]) for i in range(len(doiDF))]\n",
        "\n",
        "doiDF['year'] = year\n",
        "\n",
        "        \n",
        "        \n",
        "doiDF.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (still some cleaning: cases with empty brackets [{}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k in range(len(doiDF)):\n",
        "    if len(doiDF['affiliations'][k][0]) != 0 and doiDF['affiliations'][k][0][0] == {}:\n",
        "        print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "emptyBrackets = [k for k in range(len(doiDF)) if len(doiDF['affiliations'][k][0]) != 0 and doiDF['affiliations'][k][0][0] == {}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>items</th>\n",
              "      <th>DOI</th>\n",
              "      <th>authors</th>\n",
              "      <th># authors</th>\n",
              "      <th>affiliations</th>\n",
              "      <th># Affil</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [items, DOI, authors, # authors, affiliations, # Affil, year]\n",
              "Index: []"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doiDF.iloc[emptyBrackets]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "doiDF.drop(emptyBrackets, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "doiDF.reset_index(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "doiDF.drop(columns = ['index'], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "278"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(doiDF)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clean affiliations "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## is_contained(a,b) map : returns true when a is a substring of b "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_contained(s, w):\n",
        "    words = s.split()  # Split the string 's' into a list of words\n",
        "    for word in words:\n",
        "        if word not in w:  # If a word from 's' is not found in 'w'\n",
        "            return False  # Return False immediately\n",
        "    return True  # If all words from 's' are found in 'w', return True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. \"Unique\" affiliations --- number of unique affiliations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error indices: []\n"
          ]
        }
      ],
      "source": [
        "uniqueAff = []\n",
        "error_indices =[] # New list to store error indices\n",
        "for i in range(len(doiDF)):\n",
        "    try:\n",
        "        uniqueAff.append(list(set([x[0] for x in [list(d.values()) for d in [item for sublist in doiDF['affiliations'].iloc[i] for item in sublist if sublist !=[{}] and item !={}]]])))\n",
        "    except TypeError:\n",
        "        print(\"Error occurred for i =\", i)\n",
        "        error_indices.append(i)  # Save the index where the error occurred\n",
        "    #except IndexError:\n",
        "     #   print(\"IndexError occurred for i =\", i)\n",
        "      #  error_indices.append(i)  # Save the index where the IndexError occurred\n",
        "\n",
        "\n",
        "# Print the error indices\n",
        "print(\"Error indices:\", error_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "doiDF.drop(error_indices, inplace = True)\n",
        "doiDF.reset_index(inplace = True)\n",
        "doiDF.drop(columns = ['index'], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "doiDF.loc[:,'uniqueAff'] = uniqueAff\n",
        "\n",
        "numUniqueAff = [len(doiDF['uniqueAff'].iloc[i]) for i in range(len(doiDF))]\n",
        "\n",
        "doiDF.loc[:,'# uniqueAff'] = numUniqueAff"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Remove stop words "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "doiDF.loc[:,'uniqueAff1'] = doiDF['uniqueAff'].apply(lambda x: [s.lower() for s in x])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "stopWords = ['from', 'the', 'of', 'at', 'de','for','et','für','des', 'in','as','a','and']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_stop_words(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stopWords]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "\n",
        "# apply the function to the column  doiDF['uniqueAff'] to create column doiDF.loc[:,'uniqueAff1']\n",
        "\n",
        "doiDF.loc[:,'uniqueAff1'] = doiDF['uniqueAff'].apply(lambda x: [remove_stop_words(s) for s in x])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Remove parenthesis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_parentheses(text):\n",
        "   return re.sub(r'\\([^()]*\\)', '', text)\n",
        "\n",
        "# apply the function to each list element of column doiDF['uniqueAff1'] to remove substrings inside parentheses\n",
        "\n",
        "doiDF.loc[:,'uniqueAff1'] = doiDF['uniqueAff1'].apply(lambda x: [remove_parentheses(s) for s in x])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Remove @#$%characters and umlauts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_umlauts(text):\n",
        "    normalized_text = unicodedata.normalize('NFKD', text)\n",
        "    replaced_text = ''.join(c for c in normalized_text if not unicodedata.combining(c))\n",
        "    return replaced_text\n",
        "\n",
        "affNoSymbols = []\n",
        "\n",
        "for i in range(len(list(doiDF['uniqueAff1']))):\n",
        "    L = list(doiDF['uniqueAff1'])[i]\n",
        "    for j in range(len(L)):\n",
        "        L[j] = re.sub(r'[^\\w\\s,Α-Ωα-ωぁ-んァ-ン一-龯，]', '', L[j])\n",
        "        L[j] = L[j].replace(\"  \", \" \")\n",
        "        L[j] = replace_umlauts(L[j])\n",
        "        \n",
        "    affNoSymbols.append(L)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "affNoSymbols = [[item for item in inner_list if item != \"inc\"] for inner_list in affNoSymbols]\n",
        "\n",
        "doiDF['uniqueAff1'] = affNoSymbols"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Check 'sub'-affiliations (affiliations that are contained in other affiliations of the same DOI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "newAff0 = []\n",
        "\n",
        "for k in range(len(doiDF)):\n",
        "    \n",
        "    L2 = []\n",
        "    for s1 in doiDF['uniqueAff1'].iloc[k]:\n",
        "        is_substring = False\n",
        "        for s2 in doiDF['uniqueAff1'].iloc[k]:\n",
        "            if s1 != s2 and s1 in s2:\n",
        "                is_substring = True\n",
        "                break\n",
        "        if not is_substring:\n",
        "            L2.append(s1)\n",
        "    newAff0.append(L2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "newAffList = [list(set(newAff0[k])) for k in range(len(newAff0))]\n",
        "doiDF['Unique affiliations'] = newAffList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "allAffsList = []\n",
        "\n",
        "for doi in newAffList:\n",
        "    for aff in doi:\n",
        "        if aff not in allAffsList:\n",
        "            allAffsList.append(aff)\n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Split strings where ',' or ';' appears    | Apply .lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def substringsDict(string):\n",
        "    split_strings = [re.sub(r'^[\\s.]+|[\\s.]+$', '', s.strip()) for s in re.split(r'[,;]', string)]\n",
        "    dict_string = {}\n",
        "    index = 0\n",
        "\n",
        "    for value in split_strings:\n",
        "        if value:\n",
        "            modified_value = re.sub(r'\\buniversit\\w*', 'universit', value, flags=re.IGNORECASE)\n",
        "            dict_string[index] = modified_value.lower()\n",
        "            index += 1\n",
        "\n",
        "    return dict_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "newAffkomma = []\n",
        "\n",
        "for aff in allAffsList:\n",
        "    newAffkomma.append(substringsDict(aff))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "for dict in newAffkomma:\n",
        "    \n",
        "    if len(dict)>1:\n",
        "        for i in range(len(dict)-1):\n",
        "            if is_contained('progr', dict[i]) and is_contained('dep', dict[i+1]):\n",
        "                del dict[i]\n",
        "            elif (is_contained('assistant', dict[i]) or is_contained('researcher', dict[i]) or is_contained('phd', dict[i]) or is_contained('student', dict[i]) or is_contained('section', dict[i]) or is_contained('prof', dict[i]) or is_contained('director', dict[i])) and (not is_contained('school', dict[i+1]) or is_contained('univ', dict[i+1]) or is_contained('inst', dict[i+1]) or is_contained('lab', dict[i+1]) or is_contained('fac', dict[i+1])):\n",
        "                del dict[i]\n",
        "            elif (is_contained('engineer', dict[i]) or is_contained('progr', dict[i]) or is_contained('unit', dict[i]) or is_contained('lab', dict[i]) or is_contained('dep', dict[i]) or is_contained('inst', dict[i]) or is_contained('hosp', dict[i]) or is_contained('school', dict[i]) or is_contained('fac', dict[i])) and is_contained('univ', dict[i+1]):\n",
        "                del dict[i]\n",
        "            elif is_contained('lab', dict[i]) and (is_contained('college', dict[i+1]) or is_contained('inst', dict[i+1]) or is_contained('dep', dict[i+1]) or is_contained('school', dict[i+1])):\n",
        "                del dict[i]\n",
        "            elif is_contained('dep', dict[i]) and (is_contained('tech', dict[i+1]) or is_contained('college', dict[i+1]) or is_contained('inst', dict[i+1]) or  is_contained('hosp', dict[i+1]) or  is_contained('school', dict[i+1]) or  is_contained('fac', dict[i+1])):\n",
        "                del dict[i]\n",
        "            elif is_contained('inst',dict[i]) and (is_contained('dep', dict[i+1]) or is_contained('acad', dict[i+1]) or is_contained('hosp', dict[i+1]) or is_contained('fac', dict[i+1]) or is_contained('cent', dict[i+1]) or is_contained('div', dict[i+1])):\n",
        "                del dict[i]\n",
        "            elif is_contained('hosp',dict[i]) and is_contained('school', dict[i+1]):\n",
        "                del dict[i]\n",
        "         #   elif is_contained('hos',dict[i]) and (is_contained('cen', dict[i+1]):\n",
        "         #       del dict[i+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "lightAff = []\n",
        "for dict in newAffkomma:\n",
        "    lightAff.append(', '.join(list(dict.values())))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "removeList = ['university','research institute','laboratory' , 'universit','gmbh', 'inc', 'university of', 'research center', \n",
        "'university college','national institute of', 'school of medicine', \"university school\", 'graduate school of', 'graduate school of engineering', \n",
        "'institute of tropical medicine', 'institute of virology', 'faculty of medicine','laboratory', 'university park', 'institute of science','Polytechnic University']\n",
        "\n",
        "city_names = [\"Aberdeen\", \"Abilene\", \"Akron\", \"Albany\", \"Albuquerque\", \"Alexandria\", \"Allentown\", \"Amarillo\", \"Anaheim\", \"Anchorage\", \"Ann Arbor\", \"Antioch\", \"Apple Valley\", \"Appleton\", \"Arlington\", \"Arvada\", \"Asheville\", \"Athens\", \"Atlanta\", \"Atlantic City\", \"Augusta\", \"Aurora\", \"Austin\", \"Bakersfield\", \"Baltimore\", \"Barnstable\", \"Baton Rouge\", \"Beaumont\", \"Bel Air\", \"Bellevue\", \"Berkeley\", \"Bethlehem\", \"Billings\", \"Birmingham\", \"Bloomington\", \"Boise\", \"Boise City\", \"Bonita Springs\", \"Boston\", \"Boulder\", \"Bradenton\", \"Bremerton\", \"Bridgeport\", \"Brighton\", \"Brownsville\", \"Bryan\", \"Buffalo\", \"Burbank\", \"Burlington\", \"Cambridge\", \"Canton\", \"Cape Coral\", \"Carrollton\", \"Cary\", \"Cathedral City\", \"Cedar Rapids\", \"Champaign\", \"Chandler\", \"Charleston\", \"Charlotte\", \"Chattanooga\", \"Chesapeake\", \"Chicago\", \"Chula Vista\", \"Cincinnati\", \"Clarke County\", \"Clarksville\", \"Clearwater\", \"Cleveland\", \"College Station\", \"Colorado Springs\", \"Columbia\", \"Columbus\", \"Concord\", \"Coral Springs\", \"Corona\", \"Corpus Christi\", \"Costa Mesa\", \"Dallas\", \"Daly City\", \"Danbury\", \"Davenport\", \"Davidson County\", \"Dayton\", \"Daytona Beach\", \"Deltona\", \"Denton\", \"Denver\", \"Des Moines\", \"Detroit\", \"Downey\", \"Duluth\", \"Durham\", \"El Monte\", \"El Paso\", \"Elizabeth\", \"Elk Grove\", \"Elkhart\", \"Erie\", \"Escondido\", \"Eugene\", \"Evansville\", \"Fairfield\", \"Fargo\", \"Fayetteville\", \"Fitchburg\", \"Flint\", \"Fontana\", \"Fort Collins\", \"Fort Lauderdale\", \"Fort Smith\", \"Fort Walton Beach\", \"Fort Wayne\", \"Fort Worth\", \"Frederick\", \"Fremont\", \"Fresno\", \"Fullerton\", \"Gainesville\", \"Garden Grove\", \"Garland\", \"Gastonia\", \"Gilbert\", \"Glendale\", \"Grand Prairie\", \"Grand Rapids\", \"Grayslake\", \"Green Bay\", \"GreenBay\", \"Greensboro\", \"Greenville\", \"Gulfport-Biloxi\", \"Hagerstown\", \"Hampton\", \"Harlingen\", \"Harrisburg\", \"Hartford\", \"Havre de Grace\", \"Hayward\", \"Hemet\", \"Henderson\", \"Hesperia\", \"Hialeah\", \"Hickory\", \"High Point\", \"Hollywood\", \"Honolulu\", \"Houma\", \"Houston\", \"Howell\", \"Huntington\", \"Huntington Beach\", \"Huntsville\", \"Independence\", \"Indianapolis\", \"Inglewood\", \"Irvine\", \"Irving\", \"Jackson\", \"Jacksonville\", \"Jefferson\", \"Jersey City\", \"Johnson City\", \"Joliet\", \"Kailua\", \"Kalamazoo\", \"Kaneohe\", \"Kansas City\", \"Kennewick\", \"Kenosha\", \"Killeen\", \"Kissimmee\", \"Knoxville\", \"Lacey\", \"Lafayette\", \"Lake Charles\", \"Lakeland\", \"Lakewood\", \"Lancaster\", \"Lansing\", \"Laredo\", \"Las Cruces\", \"Las Vegas\", \"Layton\", \"Leominster\", \"Lewisville\", \"Lexington\", \"Lincoln\", \"Little Rock\", \"Long Beach\", \"Lorain\", \"Los Angeles\", \"Louisville\", \"Lowell\", \"Lubbock\", \"Macon\", \"Madison\", \"Manchester\", \"Marina\", \"Marysville\", \"McAllen\", \"McHenry\", \"Medford\", \"Melbourne\", \"Memphis\", \"Merced\", \"Mesa\", \"Mesquite\", \"Miami\", \"Milwaukee\", \"Minneapolis\", \"Miramar\", \"Mission Viejo\", \"Mobile\", \"Modesto\", \"Monroe\", \"Monterey\", \"Montgomery\", \"Moreno Valley\", \"Murfreesboro\", \"Murrieta\", \"Muskegon\", \"Myrtle Beach\", \"Naperville\", \"Naples\", \"Nashua\", \"Nashville\", \"New Bedford\", \"New Haven\", \"New London\", \"New Orleans\", \"New York\", \"New York City\", \"Newark\", \"Newburgh\", \"Newport News\", \"Norfolk\", \"Normal\", \"Norman\", \"North Charleston\", \"North Las Vegas\", \"North Port\", \"Norwalk\", \"Norwich\", \"Oakland\", \"Ocala\", \"Oceanside\", \"Odessa\", \"Ogden\", \"Oklahoma City\", \"Olathe\", \"Olympia\", \"Omaha\", \"Ontario\", \"Orange\", \"Orem\", \"Orlando\", \"Overland Park\", \"Oxnard\", \"Palm Bay\", \"Palm Springs\", \"Palmdale\", \"Panama City\", \"Pasadena\", \"Paterson\", \"Pembroke Pines\", \"Pensacola\", \"Peoria\", \"Philadelphia\", \"Phoenix\", \"Pittsburgh\", \"Plano\", \"Pomona\", \"Pompano Beach\", \"Port Arthur\", \"Port Orange\", \"Port Saint Lucie\", \"Port St. Lucie\", \"Portland\", \"Portsmouth\", \"Poughkeepsie\", \"Providence\", \"Provo\", \"Pueblo\", \"Punta Gorda\", \"Racine\", \"Raleigh\", \"Rancho Cucamonga\", \"Reading\", \"Redding\", \"Reno\", \"Richland\", \"Richmond\", \"Richmond County\", \"Riverside\", \"Roanoke\", \"Rochester\", \"Rockford\", \"Roseville\", \"Round Lake Beach\", \"Sacramento\", \"Saginaw\", \"Saint Louis\", \"Saint Paul\", \"Saint Petersburg\", \"Salem\", \"Salinas\", \"Salt Lake City\", \"San Antonio\", \"San Bernardino\", \"San Buenaventura\", \"San Diego\", \"San Francisco\", \"San Jose\", \"Santa Ana\", \"Santa Barbara\", \"Santa Clara\", \"Santa Clarita\", \"Santa Cruz\", \"Santa Maria\", \"Santa Rosa\", \"Sarasota\", \"Savannah\", \"Scottsdale\", \"Scranton\", \"Seaside\", \"Seattle\", \"Sebastian\", \"Shreveport\", \"Simi Valley\", \"Sioux City\", \"Sioux Falls\", \"South Bend\", \"South Lyon\", \"Spartanburg\", \"Spokane\", \"Springdale\", \"Springfield\", \"St. Louis\", \"St. Paul\", \"St. Petersburg\", \"Stamford\", \"Sterling Heights\", \"Stockton\", \"Sunnyvale\", \"Syracuse\", \"Tacoma\", \"Tallahassee\", \"Tampa\", \"Temecula\", \"Tempe\", \"Thornton\", \"Thousand Oaks\", \"Toledo\", \"Topeka\", \"Torrance\", \"Trenton\", \"Tucson\", \"Tulsa\", \"Tuscaloosa\", \"Tyler\", \"Utica\", \"Vallejo\", \"Vancouver\", \"Vero Beach\", \"Victorville\", \"Virginia Beach\", \"Visalia\", \"Waco\", \"Warren\", \"Washington\", \"Waterbury\", \"Waterloo\", \"West Covina\", \"West Valley City\", \"Westminster\", \"Wichita\", \"Wilmington\", \"Winston\", \"Winter Haven\", \"Worcester\", \"Yakima\", \"Yonkers\", \"York\", \"Youngstown\"]\n",
        "\n",
        "city_names = [x.lower() for x in city_names]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "for dict in newAffkomma:\n",
        "    for i in list(dict.keys()):\n",
        "\n",
        "        if dict[i] in city_names+removeList:\n",
        "            del dict[i]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "affDF = pd.DataFrame()\n",
        "affDF['Original Affiliations'] = allAffsList\n",
        "affDF['Light Affiliations'] = lightAff\n",
        "affDF['Keywords'] =  [list(d.values()) for d in newAffkomma]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Labels based on legalnames of openAIRE's organizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "uniList = ['institu', 'istitut', 'univ', 'coll', 'center','polytechnic', 'centre' , 'cnrs', 'faculty','school' , 'academy' , 'akadem','école', 'hochschule' , 'ecole', 'tech', 'observ']\n",
        "\n",
        "labList = ['lab']\n",
        "\n",
        "hosplList = ['hospital' ,'clinic', 'hôpital', 'klinik','oncol','medical']\n",
        "\n",
        "gmbhList = ['gmbh', 'company' , 'industr', 'etaireia' , 'corporation', 'inc']\n",
        "\n",
        "musList =  ['museum', 'library']\n",
        "\n",
        "foundList =  ['foundation' , 'association','organization' ,'society', 'group' ]\n",
        "\n",
        "deptList = ['district' , 'federation'  , 'government' , 'municipal' , 'county','council', 'agency']\n",
        "# miistry -> out\n",
        "\n",
        "unknownList = ['unknown']\n",
        "\n",
        "#######   Dictionaries ##########\n",
        "\n",
        "uniDict = {k: 'Univ/Inst' for k in uniList}   \n",
        "\n",
        "labDict = {k: 'Laboratory' for k in labList} \n",
        "\n",
        "hosplDict = {k: 'Hospital' for k in hosplList}   \n",
        "\n",
        "gmbhDict = {k: 'Company' for k in gmbhList}   \n",
        "\n",
        "musDict = {k: 'Museum' for k in musList}   \n",
        "\n",
        "#schoolDict = {k: 'School' for k in schoolList}   \n",
        "\n",
        "foundDict = {k: 'Foundation' for k in foundList}   \n",
        "\n",
        "deptDict = {k: 'Government' for k in deptList}   \n",
        "\n",
        "unknownDict =  {k: 'Unknown' for k in unknownList}   \n",
        "\n",
        "categDictsList = [uniDict, labDict, hosplDict, gmbhDict, musDict, #schoolDict, \n",
        "                  foundDict, deptDict, unknownDict]\n",
        "\n",
        "################# Final Dictionary #####################\n",
        "\n",
        "categDicts = {}\n",
        "i = 0\n",
        "while i in range(len(categDictsList)):\n",
        "    categDicts.update(categDictsList[i])\n",
        "    i = i+1\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## affiliationsDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "affiliationsDict = {}\n",
        "\n",
        "for i in range(len(affDF)):\n",
        "    affiliationsDict[i] = affDF['Keywords'].iloc[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "d_new = {}\n",
        "\n",
        "# iterate over the keys of affiliationsDict\n",
        "for k in range(len(affiliationsDict)):\n",
        "    # get the list associated with the current key in affiliationsDict\n",
        "    L = affiliationsDict.get(k, [])\n",
        "    mapped_listx = [[s, v] for s in L for k2, v in categDicts.items() if k2 in s]\n",
        "    \n",
        "\n",
        "    # add the mapped list to the new dictionary d_new\n",
        "    d_new[k] = mapped_listx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "affDF['Dictionary'] = list(d_new.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "notInList = [i for i in range(len(affDF)) if affDF['Dictionary'].iloc[i] == []]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "241"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(doiDF)  - len(notInList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(notInList)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# doiDF1 ['DOI', 'affiliations', 'Dictionary', 'uniqueAff2','# authors','# uniqueAff']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## New column: category based on the labels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "category = [', '.join(list(set([x[1] for x in affDF['Dictionary'].iloc[i]]))) for i in range(len(affDF))]\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "affDF.loc[:, 'Category'] = category\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### new label: rest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(affDF)):\n",
        "    if affDF['Category'].iloc[i] == '':\n",
        "        affDF.iloc[i, affDF.columns.get_loc('Category')] = 'Rest'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "affiliationsSimple = [\n",
        "    list(set([x[0] for x in affDF['Dictionary'].iloc[i]]))\n",
        "    for i in range(len(affDF))\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "affDF['Keywords'] = affiliationsSimple"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# radius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def strRadiusU(string):\n",
        "    string = string.lower()\n",
        "    radius = 3\n",
        "    \n",
        "    strList = string.split()\n",
        "    indices = []\n",
        "    result = []\n",
        "\n",
        "    for i, x in enumerate(strList):\n",
        "        if is_contained('univers',x):\n",
        "            indices.append(i)\n",
        "            \n",
        "    for r0 in indices:\n",
        "        lmin =max(0,r0-radius)\n",
        "        lmax =min(r0+radius, len(strList))\n",
        "        s = strList[lmin:lmax]\n",
        "        \n",
        "        result.append(' '.join(s))\n",
        "    \n",
        "    return result "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def strRadiusH(string):\n",
        "    string = string.lower()\n",
        "    radius = 3\n",
        "    \n",
        "    strList = string.split()\n",
        "    indices = []\n",
        "    result = []\n",
        "\n",
        "    for i, x in enumerate(strList):\n",
        "        if is_contained('hospital',x):\n",
        "            indices.append(i)\n",
        "            \n",
        "    for r0 in indices:\n",
        "        lmin =max(0,r0-radius-1)\n",
        "        lmax =min(r0+radius, len(strList))\n",
        "        s = strList[lmin:lmax]\n",
        "        \n",
        "        result.append(' '.join(s))\n",
        "    \n",
        "    return result "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "def strRadiusC(string):\n",
        "    string = string.lower()\n",
        "    radius = 2\n",
        "    \n",
        "    strList = string.split()\n",
        "    indices = []\n",
        "    result = []\n",
        "\n",
        "    for i, x in enumerate(strList):\n",
        "        if is_contained('clinic',x) or is_contained('klinik',x):\n",
        "            indices.append(i)\n",
        "            \n",
        "    for r0 in indices:\n",
        "        lmin =max(0,r0-radius-1)\n",
        "        lmax =min(r0+radius, len(strList))\n",
        "        s = strList[lmin:lmax]\n",
        "        \n",
        "        result.append(' '.join(s))\n",
        "    \n",
        "    return result "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "affiliationsSimpleN = []\n",
        "\n",
        "for i in range(len(affiliationsSimple)):\n",
        "    inner = []\n",
        "    for str in affiliationsSimple[i]:\n",
        "        if 'universit' in str:\n",
        "            for x in strRadiusU(str):\n",
        "                inner.append(x)\n",
        "        elif 'hospital' in str or 'hôpital' in str:\n",
        "            for x in strRadiusH(str):\n",
        "                inner.append(x)\n",
        "        elif 'clinic' in str or 'klinik' in str:\n",
        "            for x in strRadiusH(str):\n",
        "                inner.append(x)\n",
        "                \n",
        "        else:\n",
        "            inner.append(str)\n",
        "    \n",
        "    affiliationsSimpleN.append(inner)      \n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "affDF['Keywords'] = affiliationsSimpleN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UNIVS & LABS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "univLabs = [i for i in range(len(affDF)) if 'Laboratory' in affDF['Category'].iloc[i] \n",
        "            or 'Univ/Inst' in  affDF['Category'].iloc[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "univLabsDF = affDF.iloc[univLabs].copy()\n",
        "univLabsDF.reset_index(inplace = True)\n",
        "univLabsDF.drop(columns = ['index'], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8715203426124197"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(univLabsDF)/len(affDF)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load files from openAIRE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with open('dixOpenOrgId.pkl', 'rb') as f:\n",
        "    dixOpenOrgId = pickle.load(f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean/modify the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_key(key):\n",
        "    # Remove all non-alphanumeric characters except Greek letters and Chinese characters\n",
        "    modified_key = re.sub(r'[^\\w\\s,Α-Ωα-ωぁ-んァ-ン一-龯，]', '', key)\n",
        "    modified_key = re.sub(r'\\buniversit\\w*', 'universit', modified_key, flags=re.IGNORECASE)\n",
        "    modified_key = modified_key.replace(' and ', ' ')\n",
        "    return modified_key\n",
        "\n",
        "    \n",
        "def filter_dictionary_keys(dictionary):\n",
        "    filtered_dict = {}\n",
        "    for key, value in dictionary.items():\n",
        "        filtered_key = filter_key(key)\n",
        "        filtered_dict[filtered_key] = value\n",
        "    return filtered_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanDict(dix):\n",
        "    dix1 =  {k.replace(',', ''): v for k, v in dix.items()}\n",
        "    \n",
        "    dix1 = {replace_umlauts(key): value\n",
        "    for key, value in dix1.items()}\n",
        "    \n",
        "    dix1 = filter_dictionary_keys(dix1)\n",
        "    \n",
        "    dix2 = {}\n",
        "    \n",
        "    for key, value in dix1.items():\n",
        "        updated_key = ' '.join([word for word in key.split() if word.lower() not in stopWords])\n",
        "        dix2[updated_key] = value\n",
        "        \n",
        "    for x in list(dix2.keys()):\n",
        "        if len(x) <3:\n",
        "            del dix2[x]\n",
        "            \n",
        "    if 'universit hospital' in list(dix2.keys()):\n",
        "        del dix2['universit hospital']\n",
        "        \n",
        "    if 'universit school' in list(dix2.keys()):\n",
        "        del dix2['universit school']\n",
        "        \n",
        "    if 'ni universit' in list(dix2.keys()):\n",
        "        del dix2['ni universit']\n",
        "\n",
        "        \n",
        "    if 's v universit' in list(dix2.keys()):\n",
        "        del dix2['s v universit']\n",
        "\n",
        "    if 'k l universit' in list(dix2.keys()):\n",
        "        del dix2['k l universit']\n",
        "        \n",
        "    return dix2\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "dixOpenOrgId2 = cleanDict(dixOpenOrgId)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def findID(name):\n",
        "    lnames = []\n",
        "    for x in list(dixOpenOrgId2.keys()):\n",
        "        if name.lower() in x:\n",
        "            lnames.append(x)\n",
        "    return lnames"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MATCHINGS"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean the matchings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bestSimScore(l1, l2, l3, l4, simU, simG):\n",
        "    \"\"\"\n",
        "    Finds the best match between a 'key word' and several legal names from the OpenAIRE database.\n",
        "    ---> corrects special cases in the main map that follows\n",
        "\n",
        "    Args:\n",
        "        l1: List of light affiliations.\n",
        "        l2: number of candidates.\n",
        "        l3: List of pairs.\n",
        "        l4: mult\n",
        "\n",
        "    Returns:\n",
        "        List: Resulting list containing OpenAIRE names and their similarity scores.\n",
        "    \"\"\"\n",
        "    \n",
        "    vectorizer = CountVectorizer()\n",
        "    numUniv = sum([(l1[i].lower()).count('univ') for i in range(len(l1))])\n",
        "    result = []\n",
        "    for i in range(len(l1)):\n",
        "        best = [] \n",
        "        s = l1[i]\n",
        "\n",
        "    \n",
        "        for j in range(len(l3)):\n",
        "            x = l3[j][1] \n",
        "           \n",
        "            if [x, l3[j][2]] in result:\n",
        "                    continue\n",
        "            \n",
        "            if l4[l3[j][0]] == 1:\n",
        "               \n",
        "                if  is_contained('univ', x.lower()) and  l3[j][2]> simU:\n",
        "                    result.append([x, l3[j][2]])\n",
        "                elif  l3[j][2] >simG:\n",
        "                    result.append([x, l3[j][2]])\n",
        "\n",
        "                \n",
        "              \n",
        "            elif l3[j][2] >=0.99 and (is_contained(\"univ\", x.lower()) or is_contained(\"college\", x.lower()) or  is_contained(\"center\", x.lower()) or  is_contained(\"schule\", x.lower())): # If the similarity score of a pair (s,x) was 1, we store it to results list\n",
        "                result.append([l3[j][1], 1])\n",
        "                \n",
        "            else:\n",
        "                try:\n",
        "            #        x_contains_university = is_contained(\"university\", x.lower())\n",
        "                    if not is_contained(\"univ\", x.lower()):\n",
        "                        continue  # Skip if x does not contain \"university\" or \"univ\"\n",
        "                    \n",
        "                    if (is_contained('hosp', x.lower()) and not is_contained('hosp', s)) or (not is_contained('hosp', x.lower()) and is_contained('hosp', s)):\n",
        "                        continue\n",
        "                    s_vector = vectorizer.fit_transform([s]).toarray() #Else we compute the similarity of s with the original affiiation name\n",
        "                    x_vector = vectorizer.transform([x]).toarray()\n",
        "\n",
        "                    \n",
        "\n",
        "                    # Compute similarity between the vectors\n",
        "                    similarity = cosine_similarity(x_vector, s_vector)[0][0]\n",
        "                    if similarity> 0.1:\n",
        "\n",
        "\n",
        "                        best.append([x, similarity])#(similarity+similarity2)/2])\n",
        "                except:\n",
        "                    KeyError\n",
        "                    \n",
        "        if best:\n",
        "            max_numbers = defaultdict(float)\n",
        "            for item in best:\n",
        "                string, number = item\n",
        "                max_numbers[string] = max(max_numbers[string], number)\n",
        "\n",
        "# Create a new list with the elements having the maximum number for each string\n",
        "            reduced_best = [[string, number] for string, number in best if number == max_numbers[string]]\n",
        "\n",
        "\n",
        "            reduced_best.sort(key=lambda x: x[1], reverse=True)\n",
        " \n",
        "            result = result + reduced_best\n",
        "                \n",
        "    univ_list = []\n",
        "    other_list = []\n",
        "    \n",
        "    for r in result:\n",
        "        if is_contained('univ',r[0]):\n",
        "            univ_list.append(r)\n",
        "        else:\n",
        "            other_list.append(r)\n",
        "    \n",
        "    limit =  min(numUniv, l2)\n",
        "\n",
        "    if len(univ_list)> limit:\n",
        "        result = univ_list[:limit] + other_list\n",
        "                \n",
        "    return result"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Find rows with multiple mathcings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def index_multipleMatchings(df):\n",
        "    multipleMatchings = []\n",
        "    mult = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        result_dict = {}\n",
        "        \n",
        "\n",
        "        for t in [t[0] for t in df.Pairs.iloc[i]]:\n",
        "            key = t\n",
        "            if key in result_dict:\n",
        "                result_dict[key] += 1\n",
        "                multipleMatchings.append(i)\n",
        "                \n",
        "            else:\n",
        "                result_dict[key] = 1\n",
        "        mult.append(result_dict)\n",
        "    return [list(set(multipleMatchings)), mult]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Aff_Ids(m, DF, dixOpenAIRE, simU, simG):\n",
        "    \n",
        "    \"\"\"\n",
        "    Matches affiliations in DataFrame 'DF' with names from dictionary 'dixOpenAIRE' and their openAIRE ids based on similarity scores.\n",
        "\n",
        "    Args:\n",
        "        m (int): The number of DOIs to check.\n",
        "        DF (DataFrame): The input DataFrame containing affiliation data.\n",
        "        dixOpenAIRE (dict): A dictionary of names from OpenAIRE.\n",
        "        simU (float): Similarity threshold for universities.\n",
        "        simG (float): Similarity threshold for non-universities.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: The final DataFrame with matched affiliations and their corresponding similarity scores.\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer()\n",
        "\n",
        "    lnamelist = list(dixOpenAIRE.keys())\n",
        "    dix = {}    # will store indeces and legalnames of organizations of the DOI { i : [legalname1, legalname2,...]}\n",
        "    deiktes = []  # stores indeces where a match is found\n",
        "    similarity_ab = [] # stores lists of similarity scores of the mathces \n",
        "    pairs = [] #  pairs[i] =  [ [s,x,t] ] where (s,x) is a match and t the corresponding similarity score\n",
        "    \n",
        "    for k in range(m):\n",
        "        similar_k = []\n",
        "        pairs_k = []\n",
        "\n",
        "\n",
        "        for s in DF['Keywords'].iloc[k]:\n",
        "\n",
        "            if s in lnamelist:\n",
        "                deiktes.append(k)\n",
        "                similarity = 1\n",
        "                similar_k.append(similarity)\n",
        "                \n",
        "                pairs_k.append((s,s,similarity))\n",
        "\n",
        "                if k not in dix:\n",
        "                    dix[k] = [s]\n",
        "                else:\n",
        "                    dix[k].append(s)\n",
        "            else:\n",
        "\n",
        "                for x in lnamelist:\n",
        "                    \n",
        "                    if  is_contained(s, x):\n",
        "\n",
        "                        x_vector = vectorizer.fit_transform([x]).toarray()\n",
        "                        s_vector = vectorizer.transform([s]).toarray()\n",
        "\n",
        "                        # Compute similarity between the vectors\n",
        "                        similarity = cosine_similarity(x_vector, s_vector)[0][0]\n",
        "                        if similarity > min(simU, simG):\n",
        "                            if (is_contained('univ', s) and is_contained('univ', x)) and similarity > simU:\n",
        "                                similar_k.append(similarity)\n",
        "                                deiktes.append(k)\n",
        "                                pairs_k.append((s,x,similarity))\n",
        "\n",
        "                                if k not in dix:\n",
        "                                    dix[k] = [x]\n",
        "                                else:\n",
        "                                    dix[k].append(x)\n",
        "                            elif (not is_contained('univ', s) and not is_contained('univ', x)) and similarity > simG:\n",
        "                                similar_k.append(similarity)\n",
        "                                deiktes.append(k)\n",
        "                                pairs_k.append((s,x,similarity))\n",
        "\n",
        "                                if k not in dix:\n",
        "                                    dix[k] = [x]\n",
        "                                else:\n",
        "                                    dix[k].append(x)\n",
        "                                    \n",
        "                    elif is_contained(x, s):\n",
        "                        if (is_contained('univ', s) and is_contained('univ', x)):\n",
        "\n",
        "                            if ' and ' in s:\n",
        "                                list_s = s.split(' and ')\n",
        "                                \n",
        "                                if list_s:\n",
        "                                    for q in list_s:\n",
        "                                        if is_contained('univ', q):\n",
        "\n",
        "                                            q_vector = vectorizer.fit_transform([q]).toarray()\n",
        "                                            x_vector = vectorizer.transform([x]).toarray()\n",
        "\n",
        "                                # Compute similarity between the vectors\n",
        "                                            similarity = cosine_similarity(q_vector, x_vector)[0][0]\n",
        "                                            if similarity > simU:\n",
        "                                                similar_k.append(similarity)\n",
        "                                                deiktes.append(k)\n",
        "                                                pairs_k.append((s,x,similarity))\n",
        "\n",
        "                                                if k not in dix:\n",
        "                                                    dix[k] = [x]\n",
        "                                                else:\n",
        "                                                    dix[k].append(x)\n",
        "                            \n",
        "                            else: \n",
        "\n",
        "                                s_vector = vectorizer.fit_transform([s]).toarray()\n",
        "                                x_vector = vectorizer.transform([x]).toarray()\n",
        "\n",
        "                                # Compute similarity between the vectors\n",
        "                                similarity = cosine_similarity(s_vector, x_vector)[0][0]\n",
        "                                if similarity > simU: #max(0.82,sim):\n",
        "                                    similar_k.append(similarity)\n",
        "                                    deiktes.append(k)\n",
        "                                    pairs_k.append((s,x,similarity))\n",
        "\n",
        "                                    if k not in dix:\n",
        "                                        dix[k] = [x]\n",
        "                                    else:\n",
        "                                        dix[k].append(x)\n",
        "                        elif not is_contained('univ', s) and not is_contained('univ', x):\n",
        "  \n",
        "\n",
        "                            s_vector = vectorizer.fit_transform([s]).toarray()\n",
        "                            x_vector = vectorizer.transform([x]).toarray()\n",
        "\n",
        "                            # Compute similarity between the vectors\n",
        "                            similarity = cosine_similarity(s_vector, x_vector)[0][0]\n",
        "                            if similarity > simG: #max(0.82,sim):\n",
        "                                similar_k.append(similarity)\n",
        "                                deiktes.append(k)\n",
        "                                pairs_k.append((s,x,similarity))\n",
        "\n",
        "                                if k not in dix:\n",
        "                                    dix[k] = [x]\n",
        "                                else:\n",
        "                                    dix[k].append(x)\n",
        "                            \n",
        "        similarity_ab.append(similar_k)   \n",
        "        similarity_ab = [lst for lst in similarity_ab if lst != []]\n",
        "        pairs.append(pairs_k)\n",
        "        \n",
        " \n",
        "    \n",
        "    \n",
        "## Define the new Dataframe\n",
        "    \n",
        "    affIdDF = pd.DataFrame()\n",
        "    affIdDF['Original affiliations'] = list(DF['Original Affiliations'].iloc[list(set(deiktes))])\n",
        "\n",
        "    affIdDF['Light affiliations'] = list(DF['Light Affiliations'].iloc[list(set(deiktes))])\n",
        "\n",
        "    affIdDF['Candidates for matching'] = list(DF['Keywords'].iloc[list(set(deiktes))])\n",
        "\n",
        "\n",
        "    affIdDF['Matched openAIRE names'] = list(dix.values())\n",
        "    affIdDF['# Matched orgs'] = [len(list(dix.values())[i]) for i in range(len(list(dix.values())))]\n",
        "    \n",
        "\n",
        "    affIdDF['Similarity score'] = similarity_ab\n",
        "\n",
        "    Pairs = [lst for lst in pairs if lst]\n",
        "    affIdDF['Pairs'] = Pairs\n",
        "    affIdDF['mult'] = index_multipleMatchings(affIdDF)[1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Correct the matchings\n",
        "    needCheck = list(set([i for i in range(len(affIdDF)) for k in list(affIdDF['mult'].iloc[i].values()) if k>1]))\n",
        "    \n",
        "\n",
        "    ready = [i for i in range(len(affIdDF)) if i not in needCheck]\n",
        "    \n",
        "   \n",
        "    best = [ bestSimScore([affIdDF['Light affiliations'].iloc[i]], len(affIdDF['Candidates for matching'].iloc[i]), affIdDF['Pairs'].iloc[i],affIdDF['mult'].iloc[i], simU, simG) for i in needCheck]\n",
        "    best_o = []\n",
        "    best_s = []\n",
        "    \n",
        "    for x in best:\n",
        "        best_o.append([x[i][0]  for i in range(len(x))])\n",
        "        best_s.append([round(x[i][1],2)  for i in range(len(x))])\n",
        "    numMathced = [len(best_s[i]) for i in range(len(needCheck))]\n",
        "    \n",
        "\n",
        "    \n",
        "    dfFinal0 = (affIdDF.iloc[ready]).copy()\n",
        "    dfFinal0['index'] = ready\n",
        "    \n",
        "    dfFinal1 = (affIdDF.iloc[needCheck]).copy()\n",
        "    dfFinal1['index'] = needCheck\n",
        "    dfFinal1['Matched openAIRE names'] = best_o\n",
        "    dfFinal1['Similarity score'] = best_s\n",
        "    dfFinal1['# Matched orgs'] = numMathced\n",
        "    \n",
        "    finalDF =  pd.concat([dfFinal0, dfFinal1])\n",
        "    finalDF.set_index('index', inplace=True)\n",
        "    finalDF.sort_values('index', ascending=True, inplace = True)\n",
        "    \n",
        "    ids = [[dixOpenAIRE[x] for x in v] for v in finalDF['Matched openAIRE names']]\n",
        "    numIds = [len(x) for x in ids]\n",
        "\n",
        "    finalDF['IDs'] = ids\n",
        "    finalDF['# IDs'] = numIds\n",
        "    finalDF = finalDF[~(finalDF['# Matched orgs'] == 0)]\n",
        "    \n",
        "    finalDF = finalDF.reset_index(drop=True)\n",
        "    perc = 100*len(finalDF)/m\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    return [perc,finalDF, affIdDF, needCheck]\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = Aff_Ids(len(univLabsDF), univLabsDF, dixOpenOrgId2, 0.7,0.82)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Excel [Affiliations' matchings]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "affsMatch = result[1][['Original affiliations','Candidates for matching','Matched openAIRE names','Similarity score']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "affsMatch.to_excel('affilMatch.xlsx', index=False)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Εxcel [Dois' matchibgs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "dict_aff_open = {x: y for x, y in zip(result[1]['Original affiliations'], result[1]['Matched openAIRE names'])}\n",
        "dict_aff_id = {x: y for x, y in zip(result[1]['Original affiliations'], result[1]['IDs'])}\n",
        "dict_aff_score = {x: y for x, y in zip(result[1]['Original affiliations'], result[1]['Similarity score'])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "pids = []\n",
        "for i in range(len(doiDF)):\n",
        "    pidsi = []\n",
        "    for aff in doiDF['Unique affiliations'].iloc[i]:\n",
        "        if aff in list(dict_aff_id.keys()):\n",
        "            pidsi = pidsi + dict_aff_id[aff]\n",
        "        elif 'unmatched organization(s)' not in pidsi:\n",
        "            pidsi = pidsi + ['unmatched organization(s)']\n",
        "    pids.append(pidsi)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "names = []\n",
        "for i in range(len(doiDF)):\n",
        "    namesi = []\n",
        "    for aff in doiDF['Unique affiliations'].iloc[i]:\n",
        "        if aff in list(dict_aff_open.keys()):\n",
        "            namesi = namesi +  dict_aff_open[aff]\n",
        "        elif 'unmatched organization(s)' not in namesi:\n",
        "            namesi = namesi + ['unmatched organization(s)']\n",
        "    names.append(namesi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores = []\n",
        "for i in range(len(doiDF)):\n",
        "    scoresi = []\n",
        "    for aff in doiDF['Unique affiliations'].iloc[i]:\n",
        "        if aff in list(dict_aff_score.keys()):\n",
        "            scoresi = scoresi +  dict_aff_score[aff]\n",
        "        elif 'unmatched organization(s)' not in scoresi:\n",
        "            scoresi = scoresi + ['-']\n",
        "    scores.append(scoresi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "doiDF['Matched openAIRE names'] = names\n",
        "doiDF['IDs'] = pids\n",
        "doiDF['Scores'] = scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "unmatched = [i for i in range(len(doiDF)) if doiDF['Matched openAIRE names'].iloc[i] == ['unmatched organization(s)']]\n",
        "        \n",
        "matched = [i for i in range(len(doiDF))  if i not in unmatched]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/jt/118r6k3j6tlg5c0vfx7p74vh0000gn/T/ipykernel_34268/2561224304.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  finalDF.loc[:,'Matchings'] = matching\n"
          ]
        }
      ],
      "source": [
        "finalDF0 =  doiDF.iloc[matched]\n",
        "finalDF0.reset_index(inplace = True)\n",
        "\n",
        "finalDF = finalDF0[['DOI',\"Unique affiliations\",'Matched openAIRE names','IDs', 'Scores']]\n",
        "\n",
        "def update_Z(row):\n",
        "    new_Z = []\n",
        "    for i in range(len(row['IDs'])):\n",
        "        entry = {'OpenAIREid': row['IDs'][i], 'Confidence': row['Scores'][i]}\n",
        "        new_Z.append(entry)\n",
        "    return new_Z\n",
        "\n",
        "matching = finalDF.apply(update_Z, axis=1)\n",
        "\n",
        "finalDF.loc[:,'Matchings'] = matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "finalDF_short = finalDF[['Unique affiliations','Matched openAIRE names','Scores']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "finalDF_short.to_excel('doisMatch.xlsx', index=False)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. JSON [Final output]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "doiDF_output = finalDF[['DOI','Matchings']]\n",
        "\n",
        "doisMatch = doiDF_output.to_json(orient='records', lines=True)\n",
        "\n",
        "# Save the JSON to a file\n",
        "with open('doisMatch.json', 'w') as f:\n",
        "    f.write(doisMatch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "name": "beginners_kit_zeppelin_notebook"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
